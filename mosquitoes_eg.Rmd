---
title: "Mosquito Traps and Rainfall in YEG"
author: "Alexander Ondrus"
date: "07/03/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading & Manipulating Data

The two CSV files that we need for this example are the Rainfall Gauge Results and the Mosquitoes Trap Data. I also load the `tidyverse` library as well.

```{r Load libraries and data, message=FALSE, warning=FALSE}

```

Note that the mosquito data is _weekly_ and the rainfall data is _daily_. That means that I need to aggregate the rainfall data to the same frequency as the mosquito data and then join the two data frames together. Neither date column is immediately recognized as a date, so I use the `lubridate` package to set the format.

```{r Convert to date, message=FALSE, warning=FALSE}

```

The `lubridate` package also has a `week()` function that returns the week of the year for a given date. I will add these columns to both the mosquito and rain data. I also rename the `YEAR` column in the rain data to `Year` to match up with the mosquito data later on.

```{r Add weeks and years}

```

To simplify things greatly, I am just going to look at the total rainfall for each week and the total number of mosquitos caught each week. This means that I only need the year, week, and total columns for each data set. Note that the data is spread over multiple rows for each set, so I will need to group and summarise the data as well. To make these commands concise, I will use the pipe operator ` %>% `

```{r Select-group-summarise data}

```

Now the two data sets are on the same frequency, I can merge them using an _inner join_. This means that I only include rows that have matching values in both data frames for the specified columns.

```{r Join the data sets}

```

## Predictive Modelling

I want to predict the number of mosquitoes caught in the trap in a given week based on the total rainfall that week and the week of the year. R has many, many ways of doing this, but I will show just two. Before I do, let's take a look at the data:

```{r Visualize with 2d bin}

```


### Polynomial Regression

It is difficult to see any linear relationships in the data above, so instead I will try a polynomial regression. I do this using the `polym()` command to tell R that I want to use not only the original two variables, but their powers and intersections up to a specified degree.

```{r Polynomial Regression}

```

Taking a quick look at the legend values, it is easy to see that the polynomial regression fit does not do well at predicting the data at all.

### Regression Tree

Regression trees segment our predictor space in a manner that minimizes the variation of the output variable and uses the mean of given values to predict the output value in each region. See [here](https://en.wikipedia.org/wiki/Decision_tree_learning) for more details. The R package for building regression (or classification) trees is `tree`.

```{r Building a tree}

```

```{r Testing the tree}

```

This means that the MSE (or *M*ean *S*quared *E*rror), defined by:

$$\mathrm{MSE} = \frac{1}{n} \sum_i \left( y_{\mathrm{predicted}} - y_{\mathrm{actual}} \right)^2 $$

is equal to `r format(round(tree_mse, 0), big.mark = " ")` for our model. Taking the square root of this we can estimate that the model will be off by `r format(round(sqrt(tree_mse),0), big.mark = " ")` mosquitoes. Let's visualize the results in a way similar to linear regression.

```{r Visualizing tree predictions}

```

